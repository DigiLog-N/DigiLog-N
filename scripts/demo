#!/usr/bin/env python
from argparse import ArgumentParser
from digilog_n.DataSource import DataSource
from digilog_n.DataSourceRegistry import DataSourceRegistry
from digilog_n.EMail import email_main
from digilog_n.Log import log_file_main
from digilog_n.NotifyWriter import NotifyWriter
from digilog_n.PlasmaReader import PlasmaReader
from digilog_n.PlasmaWriter import PlasmaWriter
from digilog_n.RULResultReader import RULResultReader
from multiprocessing import Process
from os import environ, path, makedirs, getcwd, getpid, getppid
from pyspark.sql.functions import col, pandas_udf
from pyspark.sql import SparkSession
from pyspark.sql.types import LongType
from sys import stdout, argv
from time import sleep
import logging
import pandas as pd
import pyarrow as pa
import pyspark
import json
from random import randrange
from subprocess import Popen, PIPE


mylogger = logging.getLogger("mylogger")
formatter = logging.Formatter('[%(levelname)s] %(message)s')
handler = logging.StreamHandler(stream=stdout)
handler.setFormatter(formatter)
handler.setLevel(logging.INFO)
mylogger.addHandler(handler)
mylogger.setLevel(logging.INFO)
mylogger.critical("Showing")
mylogger.error("Showing")
mylogger.warning("Showing")
mylogger.info("Showing")
mylogger.debug("Showing")


class Layer(Process):
    def __init__(self):
        pass

class NotificationLayer(Layer):
    def __init__(self):
        pass

    def run(self):
        pass


def email_notification_module(msg):
    mylogger.info('Notification Module: Starting...')
    dsr = DataSourceRegistry('127.0.0.1', 27017, 'digilog_n', 'data_sources')

    ds_name = 'DigiLog-N Notifications'

    data_source = dsr.get_data_source(ds_name)

    if not data_source:
        mylogger.error("Notification Module: Could not locate data-source: %s" % ds_name)
        exit(1)

    pr = PlasmaReader(data_source.get_path_to_plasma_file(), 'NOTIFY', remove_after_reading=True)

    count = 1

    while True:
        pdf = pr.to_pandas()
        if pdf is None:
            mylogger.debug("Notification Module: No new notifications")
            pass
        else:
            mylogger.info("Notification Module: New notifications")
            pdf = pdf.sort_values(by=['epoch_timestamp'])

            pdf.to_csv('/tmp/check%d.csv' % count)
            count += 1

            user = "cowartcharles1@gmail.com"
            password = "D9AZm244L3UbYwBf"

            for index, row in pdf.iterrows():
                recipients = row['recipients'].split(',')
                subject = row['subject']
                message = row['message']

                #email_main(user, password, subject, message, recipients)
                log_file_main(user, password, subject, message, recipients)



        # sleep an arbitrary amount before checking for more notifications 
        # John and I agree that Plasma shouldn't have a problem polling at 1s intervals.
        sleep(3)


def analysis_layer(msg):
    mylogger.info('Analysis Layer: Starting...')
    dsr = DataSourceRegistry('127.0.0.1', 27017, 'digilog_n', 'data_sources')

    ds_name = 'PHM08 Prognostics Data Challenge Dataset'

    mylogger.info("Analysis Layer: Getting '%s' from Plasma..." % ds_name)

    data_source = dsr.get_data_source(ds_name)

    if not data_source:
        mylogger.error("Analysis Layer: Could not locate data-source: %s" % ds_name)
        exit(1)

    pr = PlasmaReader(data_source.get_path_to_plasma_file(), 'PHM08')

    while True:
        mylogger.info("Analysis Layer checking for new PHM08 data...")

        latest_keys = pr.get_latest_keys(mark_as_read=True)

        with open('/tmp/keys.log', 'a') as f:
            if latest_keys:
                f.write("Opening file: %d keys.\n\n" % len(latest_keys))    
                for key in latest_keys:
                    f.write("%s\n" % key)
                f.write("Closing file.\n\n")    
            else:
                f.write("Opening file: No new keys.\n\n")
                f.write("Closing file.\n\n")    

        if latest_keys:
            # There are new data points for Spark to process
            mylogger.info("Analysis Layer: Initiating Spark...")
            object_ids = ' '.join(latest_keys)
            mylogger.debug("New keys: %s" % object_ids)

            # Currently, RUL-Net does its own pulling of the data from Plasma.
            # Since additional keys may become available after we start RUL-Net,
            # We tell RUL-Net what the latest keys are, and it assembles a
            # history of each engine unit up to the point of the latest keys.
            # Anything newer should be ignored.

            # This allows us to control the lifetime of PHM08 data, as several
            # clients will be making use of it, including this program, Spark,
            # and Cassandra. 

            cmd_line = 'cd /home/charlie/RUL-Net_CL; . ./setvars.sh; . venv/bin/activate; ./run_spark.sh %s' % object_ids
            child = Popen(cmd_line, shell=True, stderr=PIPE, stdout=PIPE)
            out, err = child.communicate()
            l = out.decode('ascii').split('\n')
            l += err.decode('ascii').split('\n')
            for line in l:
                mylogger.debug(line)
        else:
            mylogger.debug("No new PHM08 data")

        sleep(1)


def fake_ims(inventory_csv_path):
    mylogger.info('Fake IMS: Starting...')
    mylogger.info('Fake IMS: Using Inventory File: %s...' % inventory_csv_path)

    parts_inventory = pd.read_csv(inventory_csv_path)
    mylogger.info(parts_inventory.head())

    dsr = DataSourceRegistry('127.0.0.1', 27017, 'digilog_n', 'data_sources')

    ds_name = 'DigiLog-N Notifications'

    data_source = dsr.get_data_source(ds_name)

    if not data_source:
        mylogger.error("Decision Maker: Could not locate data-source: %s" % ds_name)
        exit(1)

    pr = PlasmaReader(data_source.get_path_to_plasma_file(), 'INV_RQST', remove_after_reading=True)
    pw = PlasmaWriter(data_source.get_path_to_plasma_file(), 'INV_RSLT')

    while True:
        pdf = pr.to_pandas()
        if pdf is None:
            pass
        else:
            mylogger.info("New request for inventory")
            key = pw.from_pandas(parts_inventory)

        # sleep an arbitrary amount before checking for more notifications 
        sleep(3)


def decision_maker(msg):
    mylogger.info('Decision Maker: Starting...')
    dsr = DataSourceRegistry('127.0.0.1', 27017, 'digilog_n', 'data_sources')

    ds_name = 'DigiLog-N Notifications'

    data_source = dsr.get_data_source(ds_name)

    if not data_source:
        mylogger.error("Decision Maker: Could not locate data-source: %s" % ds_name)
        exit(1)

    rrr = RULResultReader(data_source.get_path_to_plasma_file(), remove_after_reading=True)
    nw = NotifyWriter(data_source.get_path_to_plasma_file())

    sleep_time = 3 

    while True:
        results = rrr.to_pandas()
        if results is None:
            mylogger.debug("No new results from Spark")
        else:
            mylogger.debug("New results from Spark - notifying...")
            for engine_unit in results:
                pdf = results[engine_unit]
                predictions = pdf.to_csv()
                message = '%s\n%s' % (engine_unit, predictions)
                with open('/tmp/sanity_check.log', 'a') as f:
                    f.write("message: %s\n" % message)
                    f.write("subject: %s\n" % 'this is a subject line')
                    f.write("\n")
                    f.write("\n")

                nw.write(['ucsdboy@gmail.com', 'unique.identifier@gmail.com'], message, 'new results from spark')

        mylogger.debug("sleeping %d seconds..." % sleep_time)
        # sleep an arbitrary amount before checking for more notifications 
        sleep(sleep_time)


if __name__ == '__main__':
    msg = 'random message'

    processes = []

    inventory_file_path = argv[1]

    processes.append(Process(target=email_notification_module, args=(msg,)))
    processes.append(Process(target=decision_maker, args=(msg,)))
    processes.append(Process(target=analysis_layer, args=(msg,)))
    processes.append(Process(target=fake_ims, args=(inventory_file_path,)))

    for p in processes:
        p.start()

    for p in processes:
        p.join()



