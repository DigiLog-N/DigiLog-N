#!/usr/bin/env python
from argparse import ArgumentParser
from digilog_n.DataSource import DataSource
from digilog_n.DataSourceRegistry import DataSourceRegistry
from digilog_n.EMail import email_main
from digilog_n.Log import log_file_main
from digilog_n.NotifyWriter import NotifyWriter
from digilog_n.AnnotatePartsReader import AnnotatePartsReader
from digilog_n.AnnotatePartsWriter import AnnotatePartsWriter
from digilog_n.AnnotateGroupsWriter import AnnotateGroupsWriter
from digilog_n.PlasmaReader import PlasmaReader
from digilog_n.PlasmaWriter import PlasmaWriter
from digilog_n.RULResultReader import RULResultReader
from multiprocessing import Process
from os import environ, path, makedirs, getcwd, getpid, getppid
from pyspark.sql.functions import col, pandas_udf
from pyspark.sql import SparkSession
from pyspark.sql.types import LongType
from sys import stdout, argv
from time import sleep
import logging
import pandas as pd
import pyarrow as pa
import pyspark
import json
from random import randrange
from subprocess import Popen, PIPE


mylogger = logging.getLogger("mylogger")
formatter = logging.Formatter('[%(levelname)s] %(message)s')
handler = logging.StreamHandler(stream=stdout)
handler.setFormatter(formatter)
handler.setLevel(logging.INFO)
mylogger.addHandler(handler)
mylogger.setLevel(logging.INFO)
mylogger.critical("Showing CRITICAL Messages")
mylogger.error("Showing ERROR Messages")
mylogger.warning("Showing WARNING Messages")
mylogger.info("Showing INFO Messages")
mylogger.debug("Showing DEBUG Messages")


class Layer(Process):
    def __init__(self):
        super().__init__()
        self.name = ''
        self.dsr = None
        self.ds_name = None
        self.plasma_path = '/tmp/plasma'

    def _before_you_begin(self):
        mylogger.info('%s Layer: Starting...' % self.name)

        #self.dsr = DataSourceRegistry('127.0.0.1', 27017, 'digilog_n', 'self.data_sources')

        #self.data_source = self.dsr.get_data_source('DigiLog-N Notifications')  #self.ds_name)

        #mylogger.info("%s Layer: Getting '%s' from Plasma..." % (self.name, self.ds_name))

        #if not self.data_source:
        #    mylogger.error("%s Layer: Could not locate data-source: %s" % (self.name, self.ds_name))
        #    exit(1)


class NotificationLayer(Layer):
    def __init__(self):
        super().__init__()
        self.name = 'Notifications'
        self.ds_name = 'DigiLog-N Notifications'

    def run(self):
        self._before_you_begin()

        pr = PlasmaReader(self.plasma_path, 'NOTIFY', remove_after_reading=True)

        count = 1

        while True:
            pdf = pr.to_pandas()
            if pdf is None:
                mylogger.debug("Notification Module: No new notifications")
                pass
            else:
                mylogger.info("Notification Module: New notifications")
                pdf = pdf.sort_values(by=['epoch_timestamp'])

                pdf.to_csv('/tmp/check%d.csv' % count)
                count += 1

                user = "cowartcharles1@gmail.com"
                password = "D9AZm244L3UbYwBf"

                for index, row in pdf.iterrows():
                    recipients = row['recipients'].split(',')
                    subject = row['subject']
                    message = row['message']

                    #email_main(user, password, subject, message, recipients)
                    log_file_main(user, password, subject, message, recipients)

            # sleep an arbitrary amount before checking for more notifications 
            # John and I agree that Plasma shouldn't have a problem polling at 1s intervals.
            sleep(3)


class AnalysisLayer(Layer):
    def __init__(self):
        super().__init__()
        self.name = 'Analysis'
        self.ds_name = 'PHM08 Prognostics Data Challenge Dataset'

    def run(self):
        self._before_you_begin()

        pr = PlasmaReader(self.plasma_path, 'PHM08')

        while True:
            mylogger.info("Analysis Layer checking for new PHM08 data...")

            latest_keys = pr.get_latest_keys(mark_as_read=True)

            with open('/tmp/keys.log', 'a') as f:
                if latest_keys:
                    f.write("Opening file: %d keys.\n\n" % len(latest_keys))    
                    for key in latest_keys:
                        f.write("%s\n" % key)
                    f.write("Closing file.\n\n")    
                else:
                    f.write("Opening file: No new keys.\n\n")
                    f.write("Closing file.\n\n")    

            if latest_keys:
                # There are new data points for Spark to process
                mylogger.info("Analysis Layer: Initiating Spark...")
                object_ids = ' '.join(latest_keys)
                mylogger.debug("New keys: %s" % object_ids)

                # Currently, RUL-Net does its own pulling of the data from Plasma.
                # Since additional keys may become available after we start RUL-Net,
                # We tell RUL-Net what the latest keys are, and it assembles a
                # history of each engine unit up to the point of the latest keys.
                # Anything newer should be ignored.

                # This allows us to control the lifetime of PHM08 data, as several
                # clients will be making use of it, including this program, Spark,
                # and Cassandra. 

                cmd_line = 'cd /home/charlie/RUL-Net_CL; . ./setvars.sh; . venv/bin/activate; ./run_spark.sh %s' % object_ids
                child = Popen(cmd_line, shell=True, stderr=PIPE, stdout=PIPE)
                out, err = child.communicate()
                l = out.decode('ascii').split('\n')
                l += err.decode('ascii').split('\n')
                for line in l:
                    mylogger.debug(line)
            else:
                mylogger.debug("No new PHM08 data")

            sleep(1)


class CassandraLayer(Layer):
    def __init__(self, inventory_csv_path):
        super().__init__()
        self.name = 'Fake IMS'
        mylogger.info('Fake IMS: Using Inventory File: %s...' % inventory_csv_path)
        self.parts_inventory = pd.read_csv(inventory_csv_path)
        mylogger.info(self.parts_inventory.head())
        self.ds_name = 'DigiLog-N Notifications'

    def run(self):
        self._before_you_begin()


class FakeIMSLayer(Layer):
    def __init__(self, inventory_csv_path):
        super().__init__()
        self.name = 'Fake IMS'
        mylogger.info('Fake IMS: Using Inventory File: %s...' % inventory_csv_path)
        self.parts_inventory = pd.read_csv(inventory_csv_path)
        mylogger.info(self.parts_inventory.head())
        self.ds_name = 'DigiLog-N Notifications'

    def run(self):
        self._before_you_begin()

        #pr = AnnotatePartsReader(self.plasma_path, remove_after_reading=True)
        pr = PlasmaReader(self.plasma_path, 'ANNO_PRTS', remove_after_reading=True)

        while True:
            mylogger.info("COOL BLUE")
            # each ANNO_PRTS key contains just a single request
            result = pr.to_pandas()
            if result is None:
                mylogger.info("No requests to annotate notifications with parts lists")
            else:
                mylogger.info("Request to annotate notifications w/parts lists")
                self.annotate(result)

            mylogger.debug("sleeping %d seconds..." % 3)

            sleep(3)

    def annotate(self, result):
        agw = AnnotateGroupsWriter(self.plasma_path)

        mylogger.info(result.head())

        part_set = {}
        part_set['CRITICAL'] = ['PartA', 'PartB', 'PartC', 'PartD', 'PartE', 'PartF', 'PartG']
        part_set['DANGER'] = ['PartA', 'PartB', 'PartC', 'PartD', 'PartE', 'PartF']
        part_set['RED'] = ['PartA', 'PartB', 'PartC', 'PartD', 'PartE']
        part_set['ORANGE'] = ['PartA', 'PartB', 'PartC', 'PartD']
        part_set['YELLOW'] = ['PartA', 'PartB', 'PartC']

        for index, row in result.iterrows():
            unit_id = row['unit_id']
            prediction = row['prediction']
            current_cycle = row['current_cycle']
            flag = row['flag']

            result = {  'unit_id': [unit_id],
                        'prediction': [prediction],
                        'current_cycle': [current_cycle],
                        'flag': [flag],
                        'part_set': [part_set[flag]] }

            agw.from_pandas(pd.DataFrame(result))


class DecisionMakerLayer(Layer):
    def __init__(self):
        super().__init__()
        self.name = 'Decision Maker'
        self.ds_name = ''

    def run(self):
        self._before_you_begin()

        rrr = RULResultReader(self.plasma_path, remove_after_reading=True)
        #nw = NotifyWriter(self.plasma_path)

        sleep_time = 3 

        while True:
            results = rrr.to_pandas()
            if results is None:
                mylogger.debug("No new results from Spark")
            else:
                for engine_unit in results:
                    self.decide(engine_unit, results[engine_unit])
                mylogger.debug("New results from Spark")

            mylogger.debug("sleeping %d seconds..." % sleep_time)
            # sleep an arbitrary amount before checking for more notifications 
            sleep(sleep_time)

    def decide(self, engine_unit, prediction_results):
        apw = AnnotatePartsWriter(self.plasma_path)

        latest_cycle = prediction_results.count()[0]
        unit_id = prediction_results.iloc[-1]['unit_id']
        prediction = prediction_results.iloc[-1]['rul_predict']

        # make a new DF with just the last row
        # append additional data to it as additional columns
        # create from scratch to bypass copy warning message.
        latest_result = {'unit_id': [unit_id], 'prediction': [prediction], 'current_cycle': [latest_cycle]}

        flag = None

        if prediction < 10:
            mylogger.info("CRITICAL: %d %d %f" % (unit_id, latest_cycle, prediction))
            flag = 'CRITICAL'
        elif prediction < 20:
            mylogger.info("DANGER: %d %d %f" % (unit_id, latest_cycle, prediction))
            flag = 'DANGER'
        elif prediction < 30:
            mylogger.info("RED: %d %d %f" % (unit_id, latest_cycle, prediction))
            flag = 'RED'
        elif prediction < 40:
            mylogger.info("ORANGE: %d %d %f" % (unit_id, latest_cycle, prediction))
            flag = 'ORANGE'
        elif prediction < 50:
            mylogger.info("YELLOW: %d %d %f" % (unit_id, latest_cycle, prediction))
            flag = 'YELLOW'

        if flag:
            latest_result['flag'] = [flag]
            latest_result = pd.DataFrame(latest_result)
            #mylogger.critical(latest_result.head())
            apw.from_pandas(latest_result)
            
            #nw.write(['ucsdboy@gmail.com', 'unique.identifier@gmail.com'], message, 'new results from spark')




if __name__ == '__main__':
    msg = 'random message'

    processes = []

    inventory_file_path = argv[1]

    processes.append(NotificationLayer())
    processes.append(DecisionMakerLayer())
    processes.append(AnalysisLayer())
    processes.append(FakeIMSLayer(inventory_file_path))

    for p in processes:
        p.start()

    for p in processes:
        p.join()



